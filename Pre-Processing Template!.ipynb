{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564ecb60",
   "metadata": {},
   "source": [
    "### Fundamental Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fff3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda39c9",
   "metadata": {},
   "source": [
    "### Loading data into data frame, Null values and Dropping columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('')\n",
    "df()\n",
    "df.describe()\n",
    "\n",
    "\n",
    "# Getting all coumn names\n",
    "df.columns\n",
    "df.columns.values\n",
    "\n",
    "# Checking for null values [Count of all null values in each columns]\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "#Dropping rows with null values present in them\n",
    "df.dropna(axis = 0)\n",
    "\n",
    "\n",
    "# Dropping an entire column(If not required)\n",
    "df.drop(['COLUMN NAME'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba3486",
   "metadata": {},
   "source": [
    "### Replacing missing values with other values!\n",
    "\n",
    "Mean, Median, Mode etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaee9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling a column value with median vlaue\n",
    "df['col1'] = df['col1'].fillna(df['col1'].median())\n",
    "\n",
    "\n",
    "\n",
    "#filling multiple columns together\n",
    "df[['col1', 'col2']] = df[['col1', 'col2']].fillna(df[['col1', 'col2']].median())\n",
    "\n",
    "\n",
    "\n",
    "#filling all values with median\n",
    "df = df.fillna(df.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb87413",
   "metadata": {},
   "source": [
    "### Dealing with Outliers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distplot [ Distribution Plot of each column!]\n",
    "\n",
    "sns.displot( df['COLUMN NAME'])\n",
    "\n",
    "\n",
    "\n",
    "# Required quantile determination 'q'based on distplot\n",
    "q1 = df['COLUMN NAME'].quantile(99)  \n",
    "q2 = df['COLUMN NAME'].quantile(1)\n",
    "\n",
    "# Setting data within range 1-99\n",
    "\n",
    "df_no_outliers = df[q2<data_no_mv['Price']<q1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524fc6f9",
   "metadata": {},
   "source": [
    "###  Handling categorical data and dummy variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96949137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To include the categorical data in the regression, let's create dummies\n",
    "# There is a very convenient method called: 'get_dummies' which does that seemlessly\n",
    "# It is extremely important that we drop one of the dummies, alternatively we will introduce multicollinearity\n",
    "df_with_dummies = pd.get_dummies(df, drop_first=True)\n",
    "df_with_dummies.head()\n",
    "\n",
    "# TO REARRANGE THE COLUMNS IN DF!\n",
    "columns = ['c1','c2','c3',....]   #Column names in desired order\n",
    "df = df_with_dummies[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a9af2",
   "metadata": {},
   "source": [
    "#### Resetting Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565160b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.reset_index(drop=True)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bcaa00",
   "metadata": {},
   "source": [
    "### Checking Relationship between two columns graphically for further analysis!\n",
    "\n",
    "Multiple Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92155e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3)) #sharey -> share 'OUTPUT COLUMN' as y\n",
    "ax1.scatter(data_cleaned['INPUT COLUMN 1'],data_cleaned['OUTPUT COLUMN'])\n",
    "ax1.set_title('INPUT COLUMN 1 and OUPUT COLUMN')\n",
    "ax2.scatter(data_cleaned['INPUT COLUMN 2'],data_cleaned['OUTPUT COLUMN'])\n",
    "ax2.set_title('INPUT COLUMN 2 and OUPUT COLUMN')\n",
    "ax3.scatter(data_cleaned['INPUT COLUMN 3'],data_cleaned['OUTPUT COLUMN'])\n",
    "ax3.set_title('INPUT COLUMN 3 and OUPUT COLUMN')\n",
    "\n",
    "\n",
    "# Here we wil get multiple plots with common Y axis and each plot with its relationship with dependent or input columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fcb9bc",
   "metadata": {},
   "source": [
    "## Statistical Data transfomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56335a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution of the COLUMN to be transformed.\n",
    "sns.distplot(data_cleaned['COLUMN'])\n",
    "\n",
    "\n",
    "# LOG transformation\n",
    "log_column = np.log(data_cleaned['COLUMN'])\n",
    "\n",
    "# Then we add LOG_COLUMN  to our data frame\n",
    "data_cleaned['log_COLUMN'] = log_COLUMN\n",
    "data_cleaned\n",
    "\n",
    "# Dropping the orignal COLUMN if not required\n",
    "\n",
    "data_cleaned.drop['COLUMN', axis = 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693455d",
   "metadata": {},
   "source": [
    "###  Feature scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaling module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "# Fit the inputs (calculate the mean and standard deviation feature-wise)\n",
    "scaler.fit(inputs)\n",
    "# Scale the features and store them in a new variable (the actual scaling procedure)\n",
    "inputs_scaled = scaler.transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e00c15",
   "metadata": {},
   "source": [
    "### Test Train split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac681954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the variables with an 80-20 split and some random state\n",
    "# To have the same split as mine, use random_state = 365\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994f6a6",
   "metadata": {},
   "source": [
    "### Create the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression object\n",
    "reg = LinearRegression()\n",
    "# Fit the regression with the scaled TRAIN inputs and targets\n",
    "reg.fit(x_train,y_train)\n",
    "\n",
    "# Let's check the outputs of the regression\n",
    "# I'll store them in y_hat as this is the 'theoretical' name of the predictions\n",
    "y_hat = reg.predict(x_train)\n",
    "\n",
    "\n",
    "\n",
    "# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot\n",
    "# The closer the points to the 45-degree line, the better the prediction\n",
    "plt.scatter(y_train, y_hat)\n",
    "# Let's also name the axes\n",
    "plt.xlabel('Targets (y_train)',size=18)\n",
    "plt.ylabel('Predictions (y_hat)',size=18)\n",
    "# Sometimes the plot will have different scales of the x-axis and the y-axis\n",
    "# This is an issue as we won't be able to interpret the '45-degree line'\n",
    "# We want the x-axis and the y-axis to be the same\n",
    "plt.xlim(6,13)\n",
    "plt.ylim(6,13)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Obtain the bias (intercept) of the regression\n",
    "reg.intercept_\n",
    "\n",
    "# Obtain the weights (coefficients) of the regression\n",
    "reg.coef_\n",
    "\n",
    "\n",
    "# Create a regression summary where we can compare them with one-another\n",
    "reg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])\n",
    "reg_summary['Weights'] = reg.coef_\n",
    "reg_summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Once we have trained and fine-tuned our model, we can proceed to testing it\n",
    "# Testing is done on a dataset that the algorithm has never seen\n",
    "# Luckily we have prepared such a dataset\n",
    "# Our test inputs are 'x_test', while the outputs: 'y_test' \n",
    "# We SHOULD NOT TRAIN THE MODEL ON THEM, we just feed them and find the predictions\n",
    "# If the predictions are far off, we will know that our model overfitted\n",
    "y_hat_test = reg.predict(x_test)\n",
    "\n",
    "\n",
    "# Create a scatter plot with the test targets and the test predictions\n",
    "# You can include the argument 'alpha' which will introduce opacity to the graph\n",
    "plt.scatter(y_test, y_hat_test, alpha=0.2)\n",
    "plt.xlabel('Targets (y_test)',size=18)\n",
    "plt.ylabel('Predictions (y_hat_test)',size=18)\n",
    "plt.xlim(6,13)\n",
    "plt.ylim(6,13)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
